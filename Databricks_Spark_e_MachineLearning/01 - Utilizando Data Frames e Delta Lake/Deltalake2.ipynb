{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução\n",
    "\n",
    "Delta Lake oferece uma camada de armazenamento que traz ACID transactions para Apache Spark e big data workloads. Esse notebook explora como realizar operações de merge, também conhecidas como upserts, que são essenciais para manter os dados atualizados sem duplicidade, usando Delta Lake no Databricks. Este exemplo prático abrange desde a leitura de dados existentes até a atualização e inserção de novos registros, culminando na remoção segura dos dados armazenados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4368cfa7-ee84-49eb-b6c3-fdaad976ffe0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação da biblioteca necessária para trabalhar com tabelas Delta, que permite realizar operações transacionais, como merge, update e delete em conjuntos de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura do DataFrame Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o DataFrame original do Delta Lake\n",
    "despachantes_df = spark.table(\"despachantes\")\n",
    "despachantes_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega o DataFrame despachantes diretamente de uma tabela Spark SQL existente e exibe suas primeiras linhas. Esta é uma forma de verificar rapidamente os dados antes de realizar operações mais complexas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparação dos Dados para Inserção/Atualização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c9eb9a0-034b-47b5-a15f-ce7ffeb7d9d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Criando um novo DataFrame com os dados que serão inseridos ou atualizados\n",
    "novo_despachante = spark.createDataFrame([(1, \"João\", \"Ativo\", \"São Paulo\", 10000, \"2023-07-05\"),\n",
    "                                          (11, \"Maria\", \"Inativo\", \"Rio de Janeiro\", 5000, \"2023-07-05\")],\n",
    "                                         [\"id\", \"nome\", \"status\", \"cidade\", \"vendas\", \"data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando e Carregando a Tabela Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificando o caminho para o diretório Delta Lake onde os dados serão armazenados\n",
    "delta_path = \"/path/despachantes\"\n",
    "\n",
    "# Salvando o DataFrame original no formato Delta Lake\n",
    "despachantes_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# Carregando o DeltaTable a partir do caminho especificado\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salva o DataFrame original no formato Delta Lake, permitindo funcionalidades transacionais e a capacidade de realizar merges. Em seguida, carregamos a tabela Delta para permitir operações de atualização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução do Merge (Upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo a condição de merge (como exemplo, vamos usar a coluna \"id\")\n",
    "condition = \"target.id = source.id\"\n",
    "\n",
    "# Executando o merge/upsert\n",
    "delta_table.alias(\"target\") \\\n",
    "    .merge(novo_despachante.alias(\"source\"), condition) \\\n",
    "    .whenMatchedUpdate(set={\"nome\": \"source.nome\", \"status\": \"source.status\", \"cidade\": \"source.cidade\",\n",
    "                            \"vendas\": \"source.vendas\", \"data\": \"source.data\"}) \\\n",
    "    .whenNotMatchedInsert(values={\"id\": \"source.id\", \"nome\": \"source.nome\", \"status\": \"source.status\",\n",
    "                                  \"cidade\": \"source.cidade\", \"vendas\": \"source.vendas\", \"data\": \"source.data\"}) \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza a operação merge para atualizar ou inserir dados na tabela Delta com base na condição especificada. Se um registro com o mesmo id já existir, ele é atualizado; caso contrário, um novo registro é inserido. Esta operação é crucial para manter a integridade dos dados sem duplicar informações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verificação dos Dados Após Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo o DataFrame resultante após o merge/upsert\n",
    "despachantes_atualizados_df = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# Exibindo o DataFrame resultante\n",
    "despachantes_atualizados_df.orderBy(\"id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega e exibe os dados da tabela Delta para verificar se o merge foi realizado corretamente, mostrando os dados ordenados por id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "241914c4-4f73-4cdb-8ff5-b9edd0ceaa42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/path/despachantes\", recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove o diretório e todos os arquivos associados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Deltalake2",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
