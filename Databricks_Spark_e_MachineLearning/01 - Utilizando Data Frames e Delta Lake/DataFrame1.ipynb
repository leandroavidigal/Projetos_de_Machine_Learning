{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução ao Databricks e Apache Spark\n",
    "\n",
    "Databricks é uma plataforma de análise baseada em nuvem otimizada para o uso do Apache Spark, que facilita o processamento de grandes conjuntos de dados e a colaboração entre usuários. Apache Spark é um sistema de computação em cluster que oferece interfaces para programação de clusters com tolerância a falhas e dados paralelos. Ele é projetado para ser rápido para consultas ad-hoc e para processamento de dados em larga escala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando DataFrames\n",
    "DataFrame é uma estrutura de dados imutável, distribuída, que permite o processamento de dados em formato tabular de maneira otimizada. Vamos iniciar criando um DataFrame simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dff9e95-b540-45e5-b0fb-65f3c10219b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|Pedro| 10|\n",
      "|Maria| 20|\n",
      "| José| 40|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "df1 = spark.createDataFrame([(\"Pedro\",10),(\"Maria\",20),(\"José\",40)])\n",
    "#show é ação, então tudo o que foi feito anteriormente é executado, lazzy\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este código acima cria um DataFrame a partir de uma lista de tuplas, onde cada tupla representa uma linha. A operação show() é usada para visualizar os dados do DataFrame. O Spark executa todas as transformações de forma lazy (preguiçosa), o que significa que ele aguarda até que uma ação como show() seja chamada para executar todas as transformações anteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo Schema Explicitamente\n",
    "\n",
    "É possível definir explicitamente o esquema dos dados ao criar um DataFrame, o que ajuda a garantir que os tipos de dados estão corretos e otimiza o desempenho do Spark:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui, definimos um schema como uma string que especifica os nomes das colunas e os tipos de dados. Após criar o DataFrame df2, utilizamos createOrReplaceTempView para registrar uma tabela temporária que pode ser utilizada em consultas SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a2f88da-c291-4155-a4b3-ddaf585c98d2",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| Id| Nome|\n",
      "+---+-----+\n",
      "|  1|Pedro|\n",
      "|  2|Maria|\n",
      "+---+-----+\n",
      "\n",
      "+---+-----+\n",
      "| Id| Nome|\n",
      "+---+-----+\n",
      "|  1|Pedro|\n",
      "+---+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#criar df com schema\n",
    "schema = \"Id INT, Nome STRING\"\n",
    "dados = [[1,\"Pedro\"],[2,\"Maria\"]]\n",
    "df2 = spark.createDataFrame(dados, schema)\n",
    "df2.show()\n",
    "df2.show(1)\n",
    "#cria tabela temporária\n",
    "df2.createOrReplaceTempView(\"df2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consultas SQL\n",
    "Com o Spark, você pode realizar consultas SQL diretamente nos DataFrames registrados como tabelas temporárias.\n",
    "Este comando SQL seleciona todas as colunas da tabela temporária df2 que criamos anteriormente. Isso ilustra como você pode interagir com DataFrames usando SQL, o que pode ser muito familiar para usuários com experiência em bancos de dados relacionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "377d51bb-3d45-425d-bbfa-2a85917934b4",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Id</th><th>Nome</th></tr></thead><tbody><tr><td>1</td><td>Pedro</td></tr><tr><td>2</td><td>Maria</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Pedro"
        ],
        [
         2,
         "Maria"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Nome",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformações e Agregações\n",
    "Você pode realizar várias transformações e agregações nos DataFrames. Por exemplo, vamos agrupar dados e calcular a soma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fafea9c4-1876-41db-84bc-0b2efc67a9f0",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n",
      "|Produtos|sum(Vendas)|\n",
      "+--------+-----------+\n",
      "|  Caneta|         50|\n",
      "|   Lápis|         20|\n",
      "+--------+-----------+\n",
      "\n",
      "+--------+-----------+\n",
      "|Produtos|sum(Vendas)|\n",
      "+--------+-----------+\n",
      "|  Caneta|         50|\n",
      "|   Lápis|         20|\n",
      "+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#com transformação\n",
    "from pyspark.sql.functions import sum\n",
    "schema2 = \"Produtos STRING, Vendas INT\"\n",
    "vendas = [[\"Caneta\",10],[\"Lápis\",20],[\"Caneta\",40]]\n",
    "df3 = spark.createDataFrame(vendas , schema2 )\n",
    "agrupado = df3.groupBy(\"Produtos\").agg(sum(\"Vendas\"))\n",
    "agrupado.show()\n",
    "#podemos contatenar as operações, neste caso sem persitir\n",
    "df3.groupBy(\"Produtos\").agg(sum(\"Vendas\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção e Manipulação de Colunas\n",
    "Além de transformações simples, o Spark também suporta a manipulação avançada de colunas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5da2c71d-64e2-4efa-a8b2-f803106f165d",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Produtos|\n",
      "+--------+\n",
      "|  Caneta|\n",
      "|   Lápis|\n",
      "|  Caneta|\n",
      "+--------+\n",
      "\n",
      "+--------+------+\n",
      "|Produtos|Vendas|\n",
      "+--------+------+\n",
      "|  Caneta|    10|\n",
      "|   Lápis|    20|\n",
      "|  Caneta|    40|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#selecionar colunas específicas\n",
    "df3.select(\"Produtos\").show()\n",
    "df3.select(\"Produtos\",\"Vendas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo, select é usado não apenas para selecionar colunas, mas também para criar uma nova coluna calculada (Vendas * 0.2), demonstrando como você pode facilmente realizar cálculos dentro de um DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621e4f52-ae90-41da-b18b-3b1b9e0b7155",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+--------------+\n",
      "|Produtos|Vendas|(Vendas * 0.2)|\n",
      "+--------+------+--------------+\n",
      "|  Caneta|    10|           2.0|\n",
      "|   Lápis|    20|           4.0|\n",
      "|  Caneta|    40|           8.0|\n",
      "+--------+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#expressões e select\n",
    "from pyspark.sql.functions import expr\n",
    "df3.select(\"Produtos\", \"Vendas\", expr(\"Vendas * 0.2\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5de53884-2104-41ec-bede-6a2ffd85b647",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('Produtos', StringType(), True), StructField('Vendas', IntegerType(), True)])\n"
     ]
    }
   ],
   "source": [
    "#ver schema\n",
    "from pyspark.sql.types import StructType\n",
    "schema = df3.schema\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dadf110d-793e-466d-9e10-3e013c961db2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Este tutorial passou pelos fundamentos de criar e manipular DataFrames no Databricks usando Apache Spark. Você aprendeu como definir DataFrames, visualizar dados, executar consultas SQL, e fazer transformações e agregações complexas. Estas são habilidades fundamentais para análise de dados em larga escala usando Spark no Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4494218772098566,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DataFrame1",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
