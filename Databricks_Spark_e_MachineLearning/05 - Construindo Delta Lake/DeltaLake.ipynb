{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8860b2f-ca91-4805-b182-e8b3d85c9ade",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Introdução ao Data Lake usando Delta Lake no Databricks\n",
    "Neste notebook, vamos explorar o processo de criação e manipulação de um Data Lake utilizando a tecnologia Delta Lake no Databricks. O Delta Lake é uma camada de armazenamento que traz confiabilidade aos data lakes, oferecendo transações ACID, manuseio de metadados unificados e escalabilidade para dados estruturados e semiestruturados. Usando arquivos CSV como ponto de partida, vamos carregar dados em formato Delta, realizar operações de leitura, inserção, e upsert, além de executar consultas SQL para extrair insights a partir dos dados.\n",
    "\n",
    "#### Carregando Arquivos CSV e Salvando como Delta\n",
    "Primeiro, carregamos os arquivos CSV, que contêm dados de várias tabelas relacionadas a produtos, clientes, empregados, detalhes de pedidos, entre outros. Após a leitura, esses dados foram armazenados no formato Delta para garantir maior consistência e desempenho.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eafb67cb-9c36-419e-abf7-0d1b548b56ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#criar delta a partir do csv\n",
    "categories = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/datalake/categories.csv\")\n",
    "categories.write.format(\"delta\").save(\"dbfs:/FileStore/tables/datalake/delta/categories.delta\")\n",
    "\n",
    "customers = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/datalake/customers.csv\")\n",
    "customers.write.format(\"delta\").save(\"dbfs:/FileStore/tables/datalake/delta/customers.delta\")\n",
    "\n",
    "employees = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/datalake/employees.csv\")\n",
    "employees.write.format(\"delta\").save(\"dbfs:/FileStore/tables/datalake/delta/employees.delta\")\n",
    "\n",
    "orderdetails = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/datalake/orderdetails.csv\")\n",
    "orderdetails.write.format(\"delta\").save(\"dbfs:/FileStore/tables/datalake/delta/orderdetails.delta\")\n",
    "\n",
    "products = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/datalake/products.csv\")\n",
    "products.write.format(\"delta\").save(\"dbfs:/FileStore/tables/datalake/delta/products.delta\")\n",
    "\n",
    "shippers = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/datalake/shippers.csv\")\n",
    "shippers.write.format(\"delta\").save(\"dbfs:/FileStore/tables/datalake/delta/shippers.delta\")\n",
    "\n",
    "suppliers = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/datalake/suppliers.csv\")\n",
    "suppliers.write.format(\"delta\").save(\"dbfs:/FileStore/tables/datalake/delta/suppliers.delta\")\n",
    "\n",
    "orders = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").option(\"inferSchema\", \"true\").load(\"dbfs:/FileStore/tables/datalake/orders.csv\")\n",
    "orders.write.format(\"delta\").save(\"dbfs:/FileStore/tables/datalake/delta/orders.delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "253f56c8-b306-490d-b0ff-f4009f3ded4c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Leitura de Dados no Formato Delta\n",
    "Com os dados armazenados no formato Delta, agora podemos ler e visualizar as tabelas. Isso nos permite aproveitar as vantagens do Delta Lake, como transações ACID e leitura rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "011b448f-e2df-4b85-bd06-c69c14fec202",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+----------+----------+--------------------+---------+------------+------------+------------+------------+\n|ProductID|         ProductName|SupplierID|CategoryID|     QuantityPerUnit|UnitPrice|UnitsInStock|UnitsOnOrder|ReorderLevel|Discontinued|\n+---------+--------------------+----------+----------+--------------------+---------+------------+------------+------------+------------+\n|        1|                Chai|         1|         1|  10 boxes x 20 bags|     18.0|          39|           0|          10|           0|\n|        2|               Chang|         1|         1|  24 - 12 oz bottles|     19.0|          17|          40|          25|           0|\n|        3|       Aniseed Syrup|         1|         2| 12 - 550 ml bottles|     10.0|          13|          70|          25|           0|\n|        4|Chef Anton's Caju...|         2|         2|      48 - 6 oz jars|     22.0|          53|           0|           0|           0|\n|        5|Chef Anton's Gumb...|         2|         2|            36 boxes|    21.35|           0|           0|           0|           1|\n|        6|Grandma's Boysenb...|         3|         2|      12 - 8 oz jars|     25.0|         120|           0|          25|           0|\n|        7|Uncle Bob's Organ...|         3|         7|     12 - 1 lb pkgs.|     30.0|          15|           0|          10|           0|\n|        8|Northwoods Cranbe...|         3|         2|     12 - 12 oz jars|     40.0|           6|           0|           0|           0|\n|        9|     Mishi Kobe Niku|         4|         6|    18 - 500 g pkgs.|     97.0|          29|           0|           0|           1|\n|       10|               Ikura|         4|         8|    12 - 200 ml jars|     31.0|          31|           0|           0|           0|\n|       11|      Queso Cabrales|         5|         4|           1 kg pkg.|     21.0|          22|          30|          30|           0|\n|       12|Queso Manchego La...|         5|         4|    10 - 500 g pkgs.|     38.0|          86|           0|           0|           0|\n|       13|               Konbu|         6|         8|            2 kg box|      6.0|          24|           0|           5|           0|\n|       14|                Tofu|         6|         7|    40 - 100 g pkgs.|    23.25|          35|           0|           0|           0|\n|       15|        Genen Shouyu|         6|         2| 24 - 250 ml bottles|     15.5|          39|           0|           5|           0|\n|       16|             Pavlova|         7|         3|    32 - 500 g boxes|    17.45|          29|           0|          10|           0|\n|       17|        Alice Mutton|         7|         6|      20 - 1 kg tins|     39.0|           0|           0|           0|           1|\n|       18|    Carnarvon Tigers|         7|         8|          16 kg pkg.|     62.5|          42|           0|           0|           0|\n|       19|Teatime Chocolate...|         8|         3|10 boxes x 12 pieces|      9.2|          25|           0|           5|           0|\n|       20|Sir Rodney's Marm...|         8|         3|       30 gift boxes|     81.0|          40|           0|           0|           0|\n+---------+--------------------+----------+----------+--------------------+---------+------------+------------+------------+------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#ler delta\n",
    "products = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/products.delta\")\n",
    "products.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "476652ad-61f4-4849-97e2-129070a2c404",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Inserção de Novos Registros no Delta Lake\n",
    "Além de apenas ler os dados, também é possível inserir novos registros em uma tabela Delta. Por exemplo, aqui adicionamos uma nova categoria no dataset categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2aa7de-d01a-4591-a9a7-6f6b1a6c4592",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#inserir no delta\n",
    "df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/categories.delta\")\n",
    "\n",
    "novacategoria = spark.createDataFrame([(9, \"coffee\", \"Moka pot, Aeropress, cappuccino\")], df.schema)\n",
    "\n",
    "novacategoria.write.format(\"delta\").mode(\"append\").save(\"dbfs:/FileStore/tables/datalake/delta/categories.delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c6d8d2b-02c7-40c7-8973-7aa0bcdae879",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+\n|CategoryID|  CategoryName|         Description|\n+----------+--------------+--------------------+\n|         1|     Beverages|Soft drinks, coff...|\n|         2|    Condiments|Sweet and savory ...|\n|         3|   Confections|Desserts, candies...|\n|         4|Dairy Products|             Cheeses|\n|         5|Grains/Cereals|Breads, crackers,...|\n|         6|  Meat/Poultry|      Prepared meats|\n|         7|       Produce|Dried fruit and b...|\n|         8|       Seafood|    Seaweed and fish|\n|         9|        coffee|Moka pot, Aeropre...|\n+----------+--------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "categories = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/categories.delta\")\n",
    "categories.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11924890-edf9-423e-b1ff-adbf3c0f2bd8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Operação de Upsert com Delta Lake\n",
    "O Delta Lake também suporta operações de upsert (inserção ou atualização), que são fundamentais para garantir a integridade dos dados em cenários de atualização contínua. Por exemplo, realizamos uma operação de upsert para atualizar ou inserir novos registros nos datasets orders e order_details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e574ccd-d60e-439e-88ab-c5c8157251d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#upsert \n",
    "from delta.tables import *\n",
    "\n",
    "# Carregar tabelas Delta como DeltaTable\n",
    "deltaTable_orders = DeltaTable.forPath(spark, \"dbfs:/FileStore/tables/datalake/delta/orders.delta\")\n",
    "deltaTable_order_details = DeltaTable.forPath(spark, \"dbfs:/FileStore/tables/datalake/delta/orderdetails.delta\")\n",
    "\n",
    "# Criar os novos registros que queremos inserir\n",
    "new_order = spark.createDataFrame([(11078, \"ALFKI\", 1, \"2023-08-01\")], [\"OrderID\", \"CustomerID\", \"EmployeeID\", \"OrderDate\"])\n",
    "new_order_details = spark.createDataFrame([(11078, 1, 18, 3)], [\"OrderID\", \"ProductID\", \"UnitPrice\", \"Quantity\"])\n",
    "\n",
    "deltaTable_orders.alias(\"orders\").merge(\n",
    "    new_order.alias(\"newOrder\"),\n",
    "    \"orders.OrderID = newOrder.OrderID\")\\\n",
    "    .whenMatchedUpdate(set = {\"CustomerID\" : \"newOrder.CustomerID\", \"EmployeeID\" : \"newOrder.EmployeeID\", \"OrderDate\" : \"newOrder.OrderDate\"})\\\n",
    "    .whenNotMatchedInsert(values = {\"OrderID\" : \"newOrder.OrderID\", \"CustomerID\" : \"newOrder.CustomerID\", \"EmployeeID\" : \"newOrder.EmployeeID\", \"OrderDate\" : \"newOrder.OrderDate\"})\\\n",
    "    .execute()\n",
    "\n",
    "deltaTable_order_details.alias(\"order_details\").merge(\n",
    "    new_order_details.alias(\"newOrderDetails\"),\n",
    "    \"order_details.OrderID = newOrderDetails.OrderID AND order_details.ProductID = newOrderDetails.ProductID\")\\\n",
    "    .whenMatchedUpdate(set = {\"UnitPrice\" : \"newOrderDetails.UnitPrice\", \"Quantity\" : \"newOrderDetails.Quantity\"})\\\n",
    "    .whenNotMatchedInsert(values = {\"OrderID\" : \"newOrderDetails.OrderID\", \"ProductID\" : \"newOrderDetails.ProductID\", \"UnitPrice\" : \"newOrderDetails.UnitPrice\", \"Quantity\" : \"newOrderDetails.Quantity\"})\\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "399b08e4-2d21-4a9e-8091-c27c594eca62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------------------+------------+-----------+-------+-------+--------+-----------+--------+----------+--------------+-----------+\n|OrderID|CustomerID|EmployeeID|          OrderDate|RequiredDate|ShippedDate|ShipVia|Freight|ShipName|ShipAddress|ShipCity|ShipRegion|ShipPostalCode|ShipCountry|\n+-------+----------+----------+-------------------+------------+-----------+-------+-------+--------+-----------+--------+----------+--------------+-----------+\n|  11078|     ALFKI|         1|2023-08-01 00:00:00|        null|       null|   null|   null|    null|       null|    null|      null|          null|       null|\n+-------+----------+----------+-------------------+------------+-----------+-------+-------+--------+-----------+--------+----------+--------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Ler a tabela Delta e filtrar por uma determinada condição\n",
    "df_orders = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/orders.delta\").filter(\"OrderID == 11078\")\n",
    "df_orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75da1d6f-5214-429b-aecb-8bdbb6e57acc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+--------+--------+\n|OrderID|ProductID|UnitPrice|Quantity|Discount|\n+-------+---------+---------+--------+--------+\n|  11078|        1|     18.0|       3|    null|\n+-------+---------+---------+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "# Ler a tabela Delta e filtrar por uma determinada condição\n",
    "df_order_details = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/orderdetails.delta\").filter(\"OrderID == 11078\").filter(\"ProductID == 1\")\n",
    "df_order_details.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ea57b7f-3cec-498b-b63b-85117a2d4013",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#criar tabela com finalidade especifica\n",
    "# Carregar as tabelas Delta em DataFrames do Spark\n",
    "df_categories = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/categories.delta\")\n",
    "df_categories.createOrReplaceTempView(\"categories\")\n",
    "\n",
    "df_products = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/products.delta\")\n",
    "df_products.createOrReplaceTempView(\"products\")\n",
    "\n",
    "df_suppliers = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/suppliers.delta\")\n",
    "df_suppliers.createOrReplaceTempView(\"suppliers\")\n",
    "\n",
    "df_employees = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/employees.delta\")\n",
    "df_employees.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "df_order_details = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/orderdetails.delta\")\n",
    "df_order_details.createOrReplaceTempView(\"order_details\")\n",
    "\n",
    "df_orders = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/orders.delta\")\n",
    "df_orders.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "df_shippers = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/shippers.delta\")\n",
    "df_shippers.createOrReplaceTempView(\"shippers\")\n",
    "\n",
    "df_customers = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/customers.delta\")\n",
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "join_query = \"\"\"\n",
    "SELECT order_details.OrderID AS OrderID, order_details.Quantity , order_details.UnitPrice as UnitPrice,\n",
    "products.ProductID as ProductID,  products.ProductName as Product, suppliers.CompanyName AS Suppliers,  \n",
    "employees.LastName as Employee, orders.OrderDate as Date, customers.CompanyName as Customer\n",
    "FROM orders\n",
    "JOIN order_details ON orders.OrderID = order_details.OrderID\n",
    "JOIN products ON order_details.ProductID = products.ProductID\n",
    "JOIN categories ON products.CategoryID = categories.CategoryID\n",
    "JOIN suppliers ON products.SupplierID = suppliers.SupplierID\n",
    "JOIN employees ON orders.EmployeeID = employees.EmployeeID\n",
    "JOIN shippers ON orders.ShipVia = shippers.ShipperID\n",
    "JOIN customers ON orders.CustomerID = customers.CustomerID\n",
    "\"\"\"\n",
    "\n",
    "df_result = spark.sql(join_query)\n",
    "\n",
    "# Escrever o resultado em uma nova tabela Delta\n",
    "df_result.write.format(\"delta\").mode(\"overwrite\").save(\"dbfs:/FileStore/tables/datalake/delta/join.delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b10b3c5-3a95-4804-aa90-1c2261b2eea5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Consultas SQL sobre Tabelas Delta\n",
    "Para extrair informações específicas, é possível criar consultas SQL diretamente sobre as tabelas Delta. Isso permite um acesso rápido e eficiente aos dados para análise e geração de relatórios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec971426-1756-4071-9fef-9db0b0509970",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+---------+---------+--------------+--------------------+--------+-------------------+--------------------+\n|OrderID|Quantity|UnitPrice|ProductID|       Product|           Suppliers|Employee|               Date|            Customer|\n+-------+--------+---------+---------+--------------+--------------------+--------+-------------------+--------------------+\n|  10248|      12|     14.0|       11|Queso Cabrales|Cooperativa de Qu...|Buchanan|2020-07-04 00:00:00|Vins et alcools C...|\n+-------+--------+---------+---------+--------------+--------------------+--------+-------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#consultar a tabela no delta com SQL\n",
    "df = spark.read.format(\"delta\").load(\"dbfs:/FileStore/tables/datalake/delta/join.delta\")\n",
    "\n",
    "df.createOrReplaceTempView(\"OrdersJoin\")\n",
    "\n",
    "results = spark.sql(\"SELECT * FROM OrdersJoin WHERE OrderID = 10248 AND ProductID =11 \")\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2341d7f-64ad-4eb7-a960-4a308869cd98",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbutils.fs.rm(\"dbfs:/FileStore/tables/datalake/delta/\", recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6468549b-0390-4537-ae1b-327d03a85a82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Conclusão\n",
    "Utilizando o Delta Lake no Databricks, conseguimos criar e manipular um Data Lake robusto, aproveitando a capacidade de realizar transações ACID, fazer upserts e consultas eficientes. Essa abordagem facilita o manuseio de grandes volumes de dados para análises avançadas e machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d71e85d-3951-41f3-a5a3-85d900f4e1a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DeltaLake",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
